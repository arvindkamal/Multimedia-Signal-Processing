{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Problem 3: NumPy CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from scipy import signal\n",
    "from imageio import imread\n",
    "from random import shuffle\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load_images\n",
    "    # Read in images and makes a list for each set in the form: [images, labels]\n",
    "    # images: np array with dims [N x img_height x img width x num_channels]\n",
    "    # labels: np array with dims [N x 1]. elephant = 0, lionfish = 1\n",
    "    #\n",
    "    # Returns:  train_set: The list [train_images, train_labels]\n",
    "    #           val_set: The list [val_images, val_labels] \n",
    "\n",
    "def load_images():\n",
    "    \n",
    "    sets = ['train', 'val']\n",
    "    \n",
    "    data_sets = []\n",
    "    for dset in sets:\n",
    "        img_path = './bin_dataset/' + dset + '/ele'\n",
    "        ele_list = [imread(os.path.join(img_path, img)) for img in os.listdir(img_path)]\n",
    "\n",
    "        img_path = './bin_dataset/' + dset + '/lio'\n",
    "        lio_list = [imread(os.path.join(img_path, img)) for img in os.listdir(img_path)]\n",
    "\n",
    "        set_images = np.stack(ele_list + lio_list)\n",
    "        N = set_images.shape[0]\n",
    "        labels = np.ones((N,1))\n",
    "        labels[0:int(N/2)] = 0\n",
    "        data_sets.append([set_images, labels])\n",
    "\n",
    "    train_set, val_set = data_sets\n",
    "\n",
    "    print(\"Loaded\", len(train_set[0]), \"training images\")\n",
    "    print(\"Loaded\", len(val_set[0]), \"validation images\")\n",
    "    \n",
    "    #to normalize all the training set and val set\n",
    "    train_set[0] = train_set[0]/255\n",
    "    val_set[0] = val_set[0]/255\n",
    "    \n",
    "    return train_set, val_set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# batchify\n",
    "#     Inputs:    train_set: List containing images and labels\n",
    "#                batch size: The desired size of each batch\n",
    "    \n",
    "#     Returns:   image_batches: A list of shuffled training image batches, each with size batch_size\n",
    "#                label_batches: A list of shuffled training label batches, each with size batch_size \n",
    "\n",
    "def batchify(train_set, batch_size):\n",
    "    \n",
    "    index = np.random.permutation(2000)\n",
    "    train_set[0], train_set[1] = train_set[0][index], train_set[1][index]\n",
    "\n",
    "    \n",
    "    image_batches = []\n",
    "    label_batches = []\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(train_set[0]), batch_size):\n",
    "            image_batches.append(train_set[0][i:i+batch_size])            \n",
    "            label_batches.append(train_set[1][i:i+batch_size])\n",
    "\n",
    "   \n",
    "\n",
    "    return image_batches, label_batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu\n",
    "    # Inputs:   x: Multi-dimensional array with size N along the first axis\n",
    "    # \n",
    "    # Returns:  out: Multi-dimensional array with same size of x \n",
    "\n",
    "def relu(x):\n",
    "    \n",
    "    out = np.maximum(0,x)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# sigmoid\n",
    "    # Inputs:    x: Multi-dimensional array with size N along the first axis\n",
    "    # \n",
    "    # Returns:   out: Multi-dimensional array with same size of x \n",
    "\n",
    "def sigmoid(x):\n",
    "    \n",
    "\n",
    "    out = 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "# unit_step\n",
    "    # Inputs:    x: Multi-dimensional array with size N along the first axis \n",
    "    # \n",
    "    # Returns:   out: Multi-dimensional array with same size of x \n",
    "\n",
    "def unit_step(x):\n",
    "    \n",
    "    out=x>0\n",
    "   \n",
    "    \n",
    "    return out "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve2D\n",
    "    # Inputs:    X: [N x height x width x num_channels]\n",
    "    #            filters: [num_filters x filter_height x filter_width x num_input_channels]\n",
    "    # \n",
    "    # Returns:   Xc: output array by convoling X and filters. [N x output_height x output_width x num_filters]\n",
    "\n",
    "def convolve2D(X0, filters):\n",
    "   \n",
    "    N, X0_len, _, num_ch = X0.shape\n",
    "    num_out_ch, filter_len, _, _ = filters.shape\n",
    "    F0_side = X0_len - filter_len + 1\n",
    "    \n",
    "    F0 = np.zeros((N, F0_side, F0_side, num_out_ch))\n",
    "    temp  = np.zeros((1,5,5,1))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for o_ch in range(num_out_ch):\n",
    "            for ch in range(num_ch):\n",
    "                \n",
    "                F0[n,:,:,o_ch] += signal.convolve2d(X0[n,:,:,ch],filters[o_ch,:,:,ch],mode='valid')\n",
    "                \n",
    "                \n",
    "#                 for f0_row in range(F0_side):\n",
    "#                     for f0_col in range(F0_side):\n",
    "#                         temp = X0[n,(0+f0_row):(filter_len+f0_row),(0+f0_col):(filter_len+f0_col),ch]*filters[o_ch,:,:,ch]\n",
    "                        \n",
    "#                         F0[n,f0_row,f0_col,o_ch] = sum(sum(temp))\n",
    "    return F0\n",
    "\n",
    "\n",
    "# maxPool\n",
    "    # Inputs:    R0: [N x height x width x num_channels]\n",
    "    #            mp_len: size of max pool window, also the stride for this MP\n",
    "    # \n",
    "    # Returns:   p_out: output of pooling R0. [N x output_height x output_width x num_channels]\n",
    "    #            R0_mask: A binary mask with the same size as R0. Indicates which index was chosen to be the max\n",
    "    #            for each max pool window. This will be used for backpropagation.\n",
    "\n",
    "def maxPool(R0, mp_len):\n",
    "\n",
    "    N, R0_len, _, num_ch = R0.shape\n",
    "    p_out_len = int((R0_len-mp_len)/mp_len + 1)\n",
    "\n",
    "    R0_mask = np.zeros(R0.shape)\n",
    "    p_out = np.zeros((N, p_out_len, p_out_len, num_ch))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for ch in range(num_ch):\n",
    "            for row in range(p_out_len): \n",
    "                for col in range(p_out_len):\n",
    "                    \n",
    "                    \n",
    "                    p_out[n,row,col,ch] = np.max(R0[n, row*mp_len:(row+1)*mp_len, col*mp_len:(col+1)*mp_len ,ch])\n",
    "                    \n",
    "                    \n",
    "                    index_row = int(np.argmax(R0[n, row*mp_len:(row+1)*mp_len, col*mp_len:(col+1)*mp_len ,ch])/mp_len)\n",
    "                    index_col = np.argmax(R0[n, row*mp_len:(row+1)*mp_len, col*mp_len:(col+1)*mp_len ,ch])%mp_len\n",
    "                    R0_mask[n, row*mp_len+index_row, col*mp_len+index_col,ch] = 1\n",
    "                    \n",
    "                    \n",
    "#                     pool = R0[n,(row*mp_len):((row+1)*mp_len),(col*mp_len):((col+1)*mp_len),ch]\n",
    "                    \n",
    "#                     p_out[n,row,col,ch] = pool.max()\n",
    "                    \n",
    "                    \n",
    "#                     ind_max = np.where(pool == pool.max()) \n",
    "                    \n",
    "#                     I0 = ind_max[0]\n",
    "#                     I1 = ind_max[1]\n",
    "                        \n",
    "                    \n",
    "#                     R0_mask[n,I0[0],I1[0],ch] = 1\n",
    "\n",
    "    \n",
    "    return p_out, R0_mask\n",
    "\n",
    "\n",
    "# fc\n",
    "    # Inputs:    X: [N x num_input_features]\n",
    "    #            W: [num_input_features x num_fc_nodes]\n",
    "    # \n",
    "    # Returns:   out: Linear combination of X and W. [N x num_fc_nodes]\n",
    "\n",
    "def fc(X, W):\n",
    "     \n",
    "    out = np.dot(X,W)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "a =np.zeros((4,5,2,3))\n",
    "# print(a)\n",
    "\n",
    "a[1,2,1,2] =10\n",
    "\n",
    "ind_max = np.where(a == a.max())\n",
    "\n",
    "# print(ind_max)\n",
    "I0 = ind_max[0]\n",
    "I1 = ind_max[1]\n",
    "I2 = ind_max[2]\n",
    "I3 = ind_max[3] \n",
    "\n",
    "# print(a[I0[0],I1[0],I2[0],I3[0]] )\n",
    "\n",
    "\n",
    "\n",
    "# print(I0[0])\n",
    "# print(I1[0])\n",
    "# print(I2[0])\n",
    "# print(I3[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnn_fwd\n",
    "    # Inputs:    X0: batch of images. [N x img_height x img_width x num_channels]\n",
    "    #            W0, W1, W2: Parameters of the CNN\n",
    "    #            mp_len: the length of one side of the max pool window\n",
    "    # \n",
    "    # Returns:   sig: vector containing the output for each sample. [N x 1]\n",
    "    #            cache: a dict containing the relevant output layer calculations that will be\n",
    "    #            used in backpropagation\n",
    "    \n",
    "def cnn_fwd(X0, W0, W1, W2, mp_len):\n",
    "    \n",
    "    # F0 \n",
    "    F0 = convolve2D(X0,W0);\n",
    "    R0 = relu(F0)\n",
    "    \n",
    "\n",
    "    # X1p \n",
    "    \n",
    "    X1p,R0_mask = maxPool(R0,mp_len)\n",
    "    \n",
    "    # X1 (flatten)\n",
    "    \n",
    "    #X1 = X1p.flatten()\n",
    "\n",
    "    X1 = X1p.reshape((X1p.shape[0],X1p.shape[1]*X1p.shape[2]*X1p.shape[3]))\n",
    "    \n",
    "    # FC Layers\n",
    "\n",
    "    F1 = fc(X1,W1)\n",
    "    X2 = relu(F1)\n",
    "    F2 = fc(X2,W2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Output\n",
    "    sig  = sigmoid(F2)\n",
    "    \n",
    "    # Save outputs of functions for backward pass\n",
    "    cache = {\n",
    "        \"F0\":F0,\n",
    "        \"R0\":R0,\n",
    "        \"X1p\":X1p,\n",
    "        \"R0m\":R0_mask,\n",
    "        \"X1\":X1,\n",
    "        \"F1\":F1,\n",
    "        \"X2\":X2,\n",
    "        \"F2\":F2      \n",
    "    }\n",
    "    \n",
    "    return sig, cache\n",
    "\n",
    "\n",
    "# loss\n",
    "    # Inputs:    sig: vector containing the CNN output for each sample. [N x 1]\n",
    "    #            Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    # \n",
    "    # Returns:   L: Loss/error criterion for the model. \n",
    "\n",
    "def loss(sig, Y):\n",
    "    \n",
    "    L = 0\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "        \n",
    "        L += -np.dot(Y[i],np.log(sig[i])) - np.dot((1-Y[i]),np.log(1-sig[i]))\n",
    "      \n",
    "    L = L/len(Y)\n",
    " \n",
    "    \n",
    "    \n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convolve2DBwd\n",
    "    # Inputs:    X0: batch of images. [N x height x width x num_channels]\n",
    "    #            dL_dF0: Gradient at the output of the conv layer. \n",
    "    # \n",
    "    # Returns:   dL_dW0. gradient of loss L wrt W0. Same size as W0\n",
    "\n",
    "def convolve2DBwd(X0, dL_dF0):\n",
    "    \n",
    "    N, X0_len, _, num_ch = X0.shape\n",
    "    _, dL_dF0_len, _, num_out_ch  = dL_dF0.shape\n",
    "    filter_len = X0_len - dL_dF0_len + 1\n",
    "    \n",
    "    dL_dW0 = np.zeros((num_out_ch, filter_len, filter_len, num_ch))\n",
    "    \n",
    "    for n in range(N):\n",
    "        for o_ch in range(num_out_ch):\n",
    "            for ch in range(num_ch):\n",
    "                dL_dW0[o_ch,:,:,ch] += signal.convolve2d(np.flip(np.flip(X0[n,:,:,ch],0),1),dL_dF0[n,:,:,ch],mode='valid')\n",
    "    \n",
    "    return dL_dW0\n",
    "\n",
    "\n",
    "# maxPoolBwd\n",
    "    # Inputs:    dL_dX1p: Gradient at the output of the MaxPool layer\n",
    "    #            R0_mask: A binary mask with the same size as R0. Defined in maxPool\n",
    "    #            mp_len: the length of one side of the max pool window\n",
    "    # \n",
    "    # Returns:   dL_dR0: Gradient at the output of ReLu\n",
    "    \n",
    "def maxPoolBwd(dL_dX1p, R0_mask,  mp_len):\n",
    "    \n",
    "    N, H, W, C = R0_mask.shape\n",
    "    N, dH, dW, C = dL_dX1p.shape\n",
    "    \n",
    "    dL_dR0 = np.zeros(R0_mask.shape)\n",
    "    \n",
    "    for n in range(N):\n",
    "        for ch in range(C):\n",
    "            for row in range(dH):\n",
    "                for col in range(dW):\n",
    "\n",
    "                    dL_dR0[n, row*mp_len:(row+1)*mp_len, col*mp_len:(col+1)*mp_len, ch] =\\\n",
    "                    R0_mask[n, row*mp_len:(row+1)*mp_len, col*mp_len:(col+1)*mp_len, ch] * dL_dX1p[n, row, col, ch]\n",
    "                    \n",
    "    return dL_dR0\n",
    "\n",
    "\n",
    "# dL_dW2\n",
    "    # Inputs:    Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    #            cache: a dict containing the relevant output layer calculations \n",
    "    # \n",
    "    # Returns:   dL_dW2: Gradient of the Loss wrt W2\n",
    "    \n",
    "def dL_dW2(Y, cache):\n",
    "   \n",
    "    dL_dW2 = np.zeros((2,1))\n",
    "    \n",
    "    F2 = cache['F2']\n",
    "    X2 = cache['X2']\n",
    "    \n",
    "\n",
    "    dL_dW2 = np.dot(X2.transpose(),(sigmoid(F2)-Y))\n",
    "    \n",
    "    dL_dW2 = dL_dW2/len(Y)\n",
    "    \n",
    "    return dL_dW2\n",
    "\n",
    "\n",
    "# dL_dW1\n",
    "    # Inputs:    Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    #            W2: Weight matrix for the second FC layer\n",
    "    #            cache: a dict containing the relevant output layer calculations \n",
    "    # \n",
    "    # Returns:   dL_dW1: Gradient of the Loss wrt W1\n",
    "    \n",
    "def dL_dW1(Y, W2, cache):\n",
    "\n",
    "    \n",
    "    \n",
    "    F2 = cache['F2']\n",
    "    F1 = cache['F1']\n",
    "    X1 = cache['X1']\n",
    "    \n",
    "    \n",
    "    dL_dF2 = np.dot((sigmoid(F2)-Y),W2.transpose())\n",
    "    \n",
    "    dL_dF1= dL_dF2*unit_step(F1)\n",
    "    \n",
    "    dL_dW1 = np.dot(X1.transpose(),dL_dF1)\n",
    " \n",
    "    dL_dW1 = dL_dW1/len(Y)\n",
    "    \n",
    "\n",
    "    \n",
    " \n",
    "    return dL_dW1\n",
    "\n",
    "\n",
    "# dL_dW0\n",
    "    # Inputs:    X0: batch of images. [N x height x width x num_channels]\n",
    "    #            Y: vector containing the ground truth label for each sample. [N x 1]\n",
    "    #            W1: Weight matrix for the first FC layer\n",
    "    #            W2: Weight matrix for the second FC layer\n",
    "    #            mp_len: the length of one side of the max pool window\n",
    "    #            cache: a dict containing the relevant output layer calculations \n",
    "    # \n",
    "    # Returns:   dL_dW0: Gradient of the Loss wrt W0\n",
    "\n",
    "def dL_dW0(X0, Y, W1, W2, mp_len, cache):\n",
    "    \n",
    "    N, X1p_len, _, no_out_ch  = cache['X1p'].shape\n",
    "    F2 = cache['F2']\n",
    "    F1 = cache['F1']\n",
    "    R0m = cache['R0m']\n",
    "    F0 = cache['F0']\n",
    "    \n",
    "    #dL_dF2\n",
    "    dL_dF2=np.dot((sigmoid(F2)-Y),W2.transpose())\n",
    "    \n",
    "    #dL_dF1\n",
    "    dL_dF1=dL_dF2*unit_step(F1)\n",
    "    \n",
    "    #dL_dX1\n",
    "    dL_dX1=np.dot(dL_dF1,W1.transpose())\n",
    "    \n",
    "    # dL_dX1p (unflatten)\n",
    "    dL_dX1p = dL_dX1.reshape((N, X1p_len,X1p_len,no_out_ch))\n",
    "    \n",
    "    # dL_dR0 (unpool)\n",
    "    \n",
    "    dL_dR0 = maxPoolBwd(dL_dX1p, R0m,  mp_len)\n",
    "    \n",
    "    # dL_dF0 (relu_bwd)\n",
    "    \n",
    "    dL_dF0 = dL_dR0*unit_step(F0)\n",
    "    \n",
    "    # dL_dW0\n",
    "    dL_dW0 = convolve2DBwd(X0, dL_dF0)\n",
    "    \n",
    "    dL_dW0 = dL_dW0 /len(Y)\n",
    "    \n",
    "    return dL_dW0\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2000 training images\n",
      "Loaded 800 validation images\n"
     ]
    }
   ],
   "source": [
    "train_set, val_set = load_images()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "epochs = 20\n",
    "lr = 0.1\n",
    "batch_size = 16\n",
    "filter_len = 5\n",
    "num_out_ch = 3\n",
    "mp_len = 12\n",
    "fc_nodes = 2\n",
    "\n",
    "# Declare weights\n",
    "W0 = np.random.randn(num_out_ch, filter_len, filter_len, 3) * 0.05\n",
    "W1 = np.random.randn(192, fc_nodes) * 0.05\n",
    "W2 = np.random.randn(fc_nodes,1) * 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.5989035507191144 train_acc: 0.70625\n"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    image_batches, label_batches = batchify(train_set, batch_size)\n",
    "    \n",
    "    num_batches = int(2000/16)\n",
    "    \n",
    "    \n",
    "    for b_idx in range(num_batches):\n",
    "        X = image_batches[b_idx]\n",
    "        Y = label_batches[b_idx]\n",
    "        \n",
    "        sig, cache = cnn_fwd(X, W0, W1, W2, mp_len)\n",
    "        \n",
    "        \n",
    "        # Calculate gradients\n",
    "        a = dL_dW2(Y, cache)\n",
    "        b = dL_dW1(Y,W2, cache)\n",
    "        c = dL_dW0(X, Y, W1, W2, mp_len, cache)\n",
    "\n",
    "        \n",
    "        # Update gradients\n",
    "        W2 = W2 - lr*a\n",
    "        W1 = W1 - lr*b\n",
    "        W0 = W0 - lr*c\n",
    "        \n",
    "    sig, _ = cnn_fwd(val_set[0], W0, W1, W2, mp_len)\n",
    "    train_acc = len(np.where(np.round(sig) == val_set[1])[0])/len(val_set[1])\n",
    "\n",
    "    print(\"train_loss:\", loss(sig, val_set[1]), \"train_acc:\", train_acc)\n",
    "        \n",
    "        \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Correctness of Forward and Backward Pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.23271396293817287 train_acc: 0.9175\n",
      "(3, 5, 5, 3)\n",
      "(192, 2)\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "weights = np.load('weights.npz')\n",
    "W0 = weights['W0']\n",
    "W1 = weights['W1']\n",
    "W2 = weights['W2']\n",
    "\n",
    "sig, _ = cnn_fwd(val_set[0], W0, W1, W2, mp_len)\n",
    "train_acc = len(np.where(np.round(sig) == val_set[1])[0])/len(val_set[1])\n",
    "\n",
    "print(\"train_loss:\", loss(sig, val_set[1]), \"train_acc:\", train_acc)\n",
    "\n",
    "print(W0.shape)\n",
    "print(W1.shape)\n",
    "print(W2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W2 value: -885.0302941176474\n",
      "W1 value: -503.03029411764714\n",
      "W0 value: -662.0302941176471\n"
     ]
    }
   ],
   "source": [
    "# Make backprop testing batch\n",
    "X_bp = np.vstack([train_set[0][0:8,:,:,:], train_set[0][-9:-1,:,:,:]])\n",
    "Y_bp = np.vstack([train_set[1][0:8], train_set[1][-9:-1]])\n",
    "\n",
    "# Initialize weights to all ones\n",
    "W0=np.ones((3, 5, 5, 3))\n",
    "W1=np.ones((192, 2))\n",
    "W2=np.ones((2, 1))\n",
    "\n",
    "sig, cache = cnn_fwd(X_bp, W0, W1, W2, mp_len)\n",
    "\n",
    "\n",
    "# Update weights once\n",
    "a = dL_dW2(Y_bp, cache)\n",
    "b = dL_dW1(Y_bp,W2, cache)\n",
    "c = dL_dW0(X_bp, Y_bp, W1, W2, mp_len, cache)\n",
    "\n",
    "W2 = W2 - lr*a\n",
    "W1 = W1 - lr*b\n",
    "W0 = W0 - lr*c\n",
    "\n",
    "\n",
    "print(\"W2 value:\", np.sum(W2))\n",
    "print(\"W1 value:\", np.sum(W1))\n",
    "print(\"W0 value:\", np.sum(W0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
