{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECE 417 Lecture 13: Baum-Welch/Expectation-Maximization for Training HMMs\n",
    "## Mark Hasegawa-Johnson, October 9, 2018\n",
    "This file is distributed under a <a href=\"https://creativecommons.org/licenses/by/3.0/\">CC-BY</a> license.  You may freely re-use or re-distribute the whole or any part.  If you re-distribute a non-trivial portion of it, give me credit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline of Today's lecture\n",
    "* Defining an HMM\n",
    "* Generating data from an HMM\n",
    "* Initializing the parameters\n",
    "* Re-estimating parameters: The forward-backward algorithm\n",
    "* The scaled forward-backward algorithm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "First let's load some libraries, and some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import requests\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining an HMM\n",
    "A diagonal covariance Gaussian Hidden Markov model (an HMM with diagonal covariance Gaussian observation probabilities) is defined by four sets of parameters:\n",
    "$$\\pi_i=\\Pr\\left\\{q_1=i\\right\\}$$\n",
    "$$a_{ij}=\\Pr\\left\\{q_{t+1}=j|q_t=i\\right\\}$$\n",
    "$$\\mu_{di}=E\\left[x_{dt}|q_t=i\\right]$$\n",
    "$$\\Sigma_{i}=E\\left[(\\vec{x}_{t}-\\vec\\mu_{i})(\\vec{x}_{t}-\\vec\\mu_{i})^T|q_t=i\\right]$$\n",
    "In this lecture I will generate data using the true values of each parameter, and then try to recognize data using estimated values of each parameter. Let's assume a three-state HMM.\n",
    "* For simplicity, let's set $\\pi_i=\\delta[i-1]$, so the HMM always starts in state 1.\n",
    "* Let's use a left-to-right HMM: $a_{ij}=0$ unless $j\\in\\left\\{i,i+1\\right\\}$.  In particular, let's use $a_{ii}=0.8$, so that the HMM is expected to stay in each state for five frames.\n",
    "* Let's use a 12-dimensional observation vector.  Its mean vector will be different in each state.\n",
    "* Let's use a variance vector which has the same variance in every dimension, but with higher variance in the middle state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [0,1,2,3]\n",
    "N = len(states)-1 # Define N as the number of emitting states\n",
    "pi_true = [1,0,0,0]\n",
    "A_true = [[0.8,0.2,0,0],[0,0.8,0.2,0],[0,0,0.8,0.2]]\n",
    "mu_true = [[2,2,2,2,0,0,0,0,0,0,0,0],[0,0,0,0,2,2,2,2,0,0,0,0],[0,0,0,0,0,0,0,0,2,2,2,2]]\n",
    "sigsq_true = [[1,1,1,1,1,1,1,1,1,1,1,1],[2,2,2,2,2,2,2,2,2,2,2,2],[1,1,1,1,1,1,1,1,1,1,1,1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generating data from an HMM\n",
    "Now let's run the HMM.  We'll generate a state $q$, then generate a vector $\\vec{x}$ from it.  Then repeat this process until $q=3$, at which point we stop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = list(np.random.choice(states,1,p=pi_true))\n",
    "q = Q[-1]\n",
    "X = []\n",
    "while q < 3:\n",
    "    xt = stats.multivariate_normal.rvs(mean=mu_true[q],cov=np.diag(sigsq_true[q]))\n",
    "    X.append(xt)\n",
    "    Q.extend(np.random.choice(states,1,p=A_true[q]))\n",
    "    q = Q[-1]\n",
    "\n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.subplot(211)\n",
    "# plt.imshow(np.transpose(X))\n",
    "# plt.title('Generated feature vectors')\n",
    "# plt.subplot(212)\n",
    "# plt.plot(Q)\n",
    "# plt.xlabel('Frames')\n",
    "# plt.title('True State Sequence')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating the parameters of an HMM\n",
    "Now let's create estimated parameters.  In order to create the initial estimate,\n",
    "we'll just divide the training example into thirds.  \n",
    "* pi: We'll assume we know this\n",
    "* A: We'll estimate this by estimating the duration of each state to be $d=T/3$, and then choosing the transition probability $a_{i,i+1}=1/d$.  That's the transition probability such that the expected duration of the state is $d$.\n",
    "* $\\mu$, $\\sigma^2$: We'll estimate these parameters as the mean and the variance, respectively, of each one-third of the given training example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi=[1,0,0,0]\n",
    "T=len(X)\n",
    "d=int(T/3)\n",
    "a=1/d\n",
    "A=[[1-a,a,0,0],[0,1-a,a,0],[0,0,1-a,a]]\n",
    "mu=[np.average(X[0:d],axis=0),np.average(X[d:2*d],axis=0),np.average(X[2*d:T],axis=0)]\n",
    "Sigma=[np.cov(X[0:d],rowvar=False)+0.2*np.identity(12),np.cov(X[d:2*d],rowvar=False)+0.2*np.identity(12),np.cov(X[2*d:T],rowvar=False)+0.2*np.identity(12)]\n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.subplot(211)\n",
    "# plt.plot(np.transpose(mu))\n",
    "# plt.legend(['state 0','state 1','state 2'])\n",
    "# plt.title('Initial estimated HMM means')\n",
    "# plt.subplot(212)\n",
    "# plt.plot(np.transpose([ np.diag(S) for S in Sigma ]))\n",
    "# plt.title('Initial estimated HMM variances')\n",
    "# plt.xlabel('Feature dimension')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.36366406,  0.79698761,  1.01818005,  2.96935425, -1.9435141 ,\n",
       "        0.09172224, -0.50924745,  0.0544484 ,  1.1343611 ,  0.58654775,\n",
       "        0.4228693 ,  1.07431116])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we'll compute the forward-backward algorithm using these initial estimates.  First we compute B, a matrix whose $(i,t)^{\\textrm{th}}$ element is $p(\\vec{x}_t|q_t=i)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E-step part 1\n",
    "$$b_i(\\vec{x}_t)=p(\\vec{x}_t|q_t=i)=\\frac{1}{(2\\pi)^{D/2}|\\Sigma_i|^{1/2}}e^{-\\frac{1}{2}(\\vec{x}_t-\\vec\\mu_{i})^T\\Sigma_i^{-1}(\\vec{x}_t-\\vec\\mu_i)}$$\n",
    "For diagonal covariance Gaussians, that's\n",
    "$$b_i(\\vec{x}_t)=\\prod_{d=1}^{12} \\frac{1}{\\sqrt{2\\pi\\sigma_i^2}}e^{-\\frac{1}{2}\\left(\\frac{x_{dt}-\\mu_{di}}{\\sigma_i}\\right)^2}$$\n",
    "Actually, I'm not sure whether full-covariance or diagonal-covariance Gaussians give better results for this assignment.  Full-covariance is a more accurate model, but it sometimes over-trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "B=np.zeros((N,T))\n",
    "for t in range(0,T):\n",
    "    for i in range(0,N):\n",
    "        B[i,t]=stats.multivariate_normal(mu[i],Sigma[i]).pdf(X[t])\n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.imshow(np.log(B))\n",
    "# plt.title('Log probability of the observation given the state')\n",
    "# plt.xlabel('Frame number')\n",
    "# plt.ylabel('State number')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### E-Step part 2\n",
    "Now we compute alpha, beta, gamma, and xi.\n",
    "$$\\alpha_0(i)=\\Pr\\left\\{q_0=i,\\vec{x}_0\\right\\}=\\pi_i b_i(\\vec{x}_0)$$\n",
    "$$\\alpha_t(i)=\\Pr\\left\\{q_t=i,\\vec{x}_0,\\ldots,\\vec{x}_t\\right\\}=b_i(\\vec{x}_t)\\sum_{j=0}^{N-1}\\alpha_{t-1}(j)a_{ji}$$\n",
    "$$\\beta_{T-1}(i)=1$$\n",
    "$$\\beta_{t}(i)=\\Pr\\left\\{\\vec{x}_{t+1},\\ldots,\\vec{x}_{T-1}|q_t=i\\right\\}\\sum_{j=0}^{N-1}a_{ij}b_j(\\vec{x}_{t+1})\\beta_{t+1}(j)$$\n",
    "$$\\gamma_t(i)=\\Pr\\left\\{q_t=i|\\vec{x}_0,\\ldots,\\vec{x}_{T-1}\\right\\}=\\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{j=0}^{N-1}\\alpha_t(j)\\beta_t(j)}$$\n",
    "$$\\xi_t(i,j)=\\Pr\\left\\{q_t=i,q_{t+1}=j|\\vec{x}_0,\\ldots,\\vec{x}_{T-1}\\right\\}=\\frac{\\alpha_t(i)a_{ij}b_j(\\vec{x}_{t+1})\\beta_{t+1}(j)}{\\sum_{i=0}^{N-1}\\sum_{j=0}^{N-1}\\alpha_t(i)a_{ij}b_j(\\vec{x}_{t+1})\\beta_{t+1}(j)}$$\n",
    "\n",
    "Notice that, since $a_{ij}=0$ except for $j\\in\\left\\{i,i+1\\right\\}$, therefore $\\xi_t(i,j)=0$ except for $j\\in\\left\\{i,i+1\\right\\}$.  Therefore we can save a little space in computations by computing $\\xi_t(i,j)$ only for those two possibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = np.zeros((N,T))\n",
    "beta = np.zeros((N,T))\n",
    "gamma = np.zeros((N,T))\n",
    "xi = np.zeros((2*N,T))\n",
    "Amat = np.array(A)  # Convert to an np matrix so we can compute inner products\n",
    "for i in range(0,N):\n",
    "    alpha[i,0]=pi[i]*B[i,0]\n",
    "for t in range(1,T):\n",
    "    for i in range(0,N):\n",
    "        alpha[i,t]=B[i,t]*np.inner(alpha[:,t-1],Amat[:,i])\n",
    "for i in range(0,N):\n",
    "    beta[i,T-1]=1\n",
    "for t in range(T-2,-1,-1):\n",
    "    for i in range(0,N):\n",
    "        beta[i,t]=np.inner(Amat[i,0:N],beta[:,t+1]*B[:,t+1])\n",
    "for t in range(0,T):\n",
    "    gamma[:,t]=alpha[:,t]*beta[:,t]\n",
    "    gamma[:,t]=gamma[:,t]/np.sum(gamma[:,t])\n",
    "for t in range(0,T):\n",
    "    for i in range(0,N):\n",
    "        for j in range(i,i+2):\n",
    "            xi[i+j,t]=alpha[i,t]*Amat[i,j]\n",
    "            if (t<T-1):\n",
    "                if j==N:\n",
    "                    xi[i+j,t]=0\n",
    "                else:\n",
    "                    xi[i+j,t] = xi[i+j,t]*B[j,t+1]*beta[j,t+1]\n",
    "    xi[:,t]=xi[:,t]/np.sum(xi[:,t])\n",
    "    \n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.subplot(311)\n",
    "# plt.plot(range(0,T),np.transpose(np.log(alpha)),range(0,T),np.transpose(np.log(beta)))\n",
    "# plt.title('Log Alpha and Log Beta')\n",
    "# plt.legend(['state 0','state 1','state 2'])\n",
    "# plt.subplot(312)\n",
    "# plt.plot(np.transpose(gamma))\n",
    "# plt.title('Gamma')\n",
    "# plt.subplot(313)\n",
    "# plt.plot(np.transpose(xi))\n",
    "# plt.legend(['0->0','0->1','1->1','1->2','2->2','2->3'])\n",
    "# plt.title('Xi')\n",
    "# plt.xlabel('Frame index, t')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### M-Step for multiple files\n",
    "If you have multiple files, then you compute for example $\\gamma_{t\\ell}(i)$ for the $t^{th}$ frame of the $\\ell^{th}$ file\n",
    "$$a_{ij}=\\frac{\\sum_\\ell\\sum_t \\xi_{t\\ell}(i,j)}{\\sum_\\ell\\sum_t\\gamma_{t\\ell}(i)}\\approx \\Pr\\left\\{q_{t+1}=j|q_t=i\\right\\}$$\n",
    "$$\\vec\\mu_i=\\frac{\\sum_\\ell\\sum_t\\gamma_{t\\ell}(i)\\vec{x}_t}{\\sum_\\ell\\sum_t\\gamma_{t\\ell}(i)}\\approx E\\left[\\vec{x}_{t}|q_t=i\\right]$$\n",
    "$$\\sigma_{di}^2=\\frac{\\sum_\\ell\\sum_t\\gamma_{t\\ell}(i)(x_{dt}-\\mu_{di})^2}{\\sum_\\ell\\sum_t\\gamma_{t\\ell}(i)}\\approx E\\left[(x_{dt}-\\mu_{di})^2|q_t=i\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,N):\n",
    "    for j in range(i,i+2):\n",
    "        A[i][j]=np.sum(xi[i+j,:])/np.sum(gamma[i,:])\n",
    "for i in range(0,N):\n",
    "    mu[i] = np.inner(np.transpose(X),gamma[i,:])/np.sum(gamma[i,:])\n",
    "for i in range(0,N):\n",
    "    Sigma[i]=0.2*np.identity(12)\n",
    "    for t in range(0,len(X)):\n",
    "        Sigma[i] += gamma[i,t]*np.outer(X[t]-mu[i],X[t]-mu[i])\n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.subplot(311)\n",
    "# plt.plot(np.transpose(A))\n",
    "# plt.title('Re-estimated Transition Probabilities')\n",
    "# plt.subplot(312)\n",
    "# plt.plot(np.transpose(mu))\n",
    "# plt.title('Re-estimated Gaussian Means')\n",
    "# plt.legend(['state 0','state 1','state 2'])\n",
    "# plt.subplot(313)\n",
    "# plt.plot(np.transpose([np.diag(S) for S in Sigma]))\n",
    "# plt.title('Re-estimated Gaussian Variances')\n",
    "# plt.xlabel('Feature Dimension')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Iterating the EM algorithm\n",
    "In real life, we would iterate the above algorithm several times between the E-step and the M-step.  We won't do that today."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scaled EM\n",
    "You might have noticed that $\\alpha_t(i)$ keeps getting smaller and smaller as you go further to the right.  That's because the Gaussian pdf can be either bigger or smaller than 1.0, but usually it's much smaller.\n",
    "\n",
    "Suppose that $b_j(\\vec{x}_t)$ is typically around $0.001$.  Then $\\alpha_t(i)\\sim 10^{-3t}\\sim 2^{-30t}$.  Standard IEEE double-precision floating point can represent numbers as small as $2^{-1022}$, which means that you can compute the forward-backward algorithm for an audio file of at most $T\\le 1022/30=34$ frames.  The problem is much worse on fixed-point embedded processors (obviously).\n",
    "\n",
    "The solution is to scale $\\alpha_t(i)$ and $\\beta_t(i)$.  As long as we scale them by the same amount, then we can compute $\\gamma_t(i)$ without any special re-normalizing.  It works like this:\n",
    "$$\\tilde{\\alpha}_t(i)=\\frac{\\alpha_t(i)}{\\prod_{\\tau=1}^t g_\\tau}$$\n",
    "$$\\tilde{\\beta}_t(i)=\\frac{\\beta_t(i)}{\\prod_{\\tau=t+1}^T g_\\tau}$$\n",
    "$$\\gamma_t(i)=\\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{j=0}^{N-1}\\alpha_t(j)\\beta_t(j)}$$\n",
    "$$=\\frac{\\frac{\\alpha_t(i)\\beta_t(i)}{\\prod_{\\tau=1}^T g_\\tau}}{\\frac{\\sum_{j=0}^{N-1}\\alpha_t(j)\\beta_t(j)}{\\prod_{\\tau=1}^T g_\\tau}}$$\n",
    "$$=\\frac{\\tilde{\\alpha}_t(i)\\tilde{\\beta}_t(i)}{\\sum_{j=0}^{N-1}\\tilde{\\alpha}_t(j)\\tilde{\\beta}_t(j)}$$\n",
    "\n",
    "So any scaling constant $g_t$ will work, AS LONG AS YOU USE THE SAME $g_t$ FOR BOTH ALPHA AND BETA!!!\n",
    "\n",
    "Here's a pretty reasonable choice:\n",
    "$$\\bar\\alpha_t(i)=b_i(\\vec{x}_t)\\sum_{j=0}^{N-1}\\tilde{\\alpha}_{t-1}(j)a_{ji}$$\n",
    "$$g_t=\\sum_{i=0}^{N-1}\\bar\\alpha_t(i)$$\n",
    "$$\\tilde\\alpha_t(i) = \\frac{1}{g_t}\\bar\\alpha_t(i)$$\n",
    "\n",
    "If you work through it, you discover that, with this choice,\n",
    "$$\\prod_{t=1}^T g_t = \\sum_{j=0}^{N-1}\\alpha_T(j)$$\n",
    "\n",
    "But you might remember that $\\sum_{j=0}^{N-1}\\alpha_T(j)$ is actually the probability of the data sequence, $X$, given the model parameters $\\Lambda$.  So\n",
    "$$p(X|\\Lambda)=\\sum_{j=0}^{N-1}\\alpha_T(j)=\\prod_{t=1}^T g_t$$\n",
    "$$\\ln p(X|\\Lambda)=\\sum_{t=1}^{T}\\ln g_t$$\n",
    "\n",
    "So we don't need to store $g_t$, just $\\ln g_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tildealpha=np.zeros((N,T))\n",
    "tildebeta=np.zeros((N,T))\n",
    "log_g = np.zeros((T))\n",
    "baralpha = np.zeros((N,T))\n",
    "Amat = np.array(A)\n",
    "\n",
    "for i in range(0,N):\n",
    "    baralpha[i,0]=pi[i]*B[i,0]\n",
    "log_g[0] = np.log(np.sum(baralpha[:,0]))\n",
    "tildealpha[:,0]=baralpha[:,0]/np.exp(log_g[0])\n",
    "\n",
    "for t in range(1,T):\n",
    "    for i in range(0,N):\n",
    "        baralpha[i,t]=B[i,t]*np.inner(tildealpha[:,t-1],Amat[:,i])\n",
    "    log_g[t] = np.log(np.sum(baralpha[:,t]))\n",
    "    tildealpha[:,t]=baralpha[:,t]/np.exp(log_g[t])\n",
    "\n",
    "for i in range(0,N):\n",
    "    tildebeta[i,T-1] = 1/np.exp(log_g[T-1])\n",
    "\n",
    "for t in range(T-2,-1,-1):\n",
    "    for i in range(0,N):\n",
    "        tildebeta[i,t]=np.inner(Amat[i,0:N],tildebeta[:,t+1]*B[:,t+1])/np.exp(log_g[t+1])\n",
    "\n",
    "for t in range(0,T):\n",
    "    gamma[:,t] = tildealpha[:,t]*tildebeta[:,t]\n",
    "    gamma[:,t] = gamma[:,t]/np.sum(gamma[:,t])\n",
    "    \n",
    "# plt.figure(figsize=(15,5))\n",
    "# plt.subplot(311)\n",
    "# plt.plot(log_g)\n",
    "# plt.title('Log Gain at each Frame')\n",
    "# plt.subplot(312)\n",
    "# plt.plot(np.transpose(tildealpha))\n",
    "# plt.title('Tilde-Alpha')\n",
    "# plt.legend(['state 0','state 1','state 2'])\n",
    "# plt.subplot(313)\n",
    "# plt.plot(np.transpose(gamma))\n",
    "# plt.title('Gamma')\n",
    "# plt.xlabel('Frame Index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
