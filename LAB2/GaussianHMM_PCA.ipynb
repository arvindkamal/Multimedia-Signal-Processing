{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Problem #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy import linalg\n",
    "import math\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import scipy.stats as stats\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_reduce(signal, n_components, block_size=1024):\n",
    "    \n",
    "    # First, zero-pad the signal so that it is divisible by the block_size\n",
    "    samples = len(signal)\n",
    "    hanging = block_size - np.mod(samples, block_size)\n",
    "    padded = np.lib.pad(signal, (0, hanging), 'constant', constant_values=0)\n",
    "    \n",
    "    # Reshape the signal to have 1024 dimensions\n",
    "    reshaped = padded.reshape((len(padded) // block_size, block_size))\n",
    "    \n",
    "    # Second, do the actual PCA process\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(reshaped)\n",
    "    \n",
    "    comps = pca.components_\n",
    "    transformed = pca.transform(reshaped)\n",
    "    reconstructed = pca.inverse_transform(transformed).reshape((len(padded)))\n",
    "    return pca, comps, reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "data= {}\n",
    "fs={}\n",
    "peeps=['dg', 'ls', 'mh', 'yx']\n",
    "words=['asr', 'cnn', 'dnn', 'hmm', 'tts']\n",
    "features= defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "\n",
    "for x in range(len(peeps)):\n",
    "    p=peeps[x]\n",
    "    for y in range(len(words)):\n",
    "        w=words[y]\n",
    "        for i in range(1, 6):\n",
    "            wav_r=wave.open('data/'+p+'/'+p+'_'+w+str(i)+'.wav', 'rb')\n",
    "            fs[p, w, i], x = wavfile.read('data/'+p+'/'+p+'_'+w+str(i)+'.wav')\n",
    "            if wav_r.getnchannels()==2:\n",
    "                data[p, w, i]=x[:, 0]\n",
    "                #print(x[:,0],'\\nasasas\\n')\n",
    "                _, comps, reconstructed = pca_reduce(x[:, 0], 14, 1024)\n",
    "            else:\n",
    "                data[p, w, i]=x\n",
    "                _, comps, reconstructed = pca_reduce(x, 14, 1024)\n",
    "            features[p][w][i]=comps\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features= defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "# peeps=['dg', 'ls', 'mh', 'yx']\n",
    "# words=['asr', 'cnn', 'dnn', 'hmm', 'tts']\n",
    "\n",
    "# for p in range(len(peeps)):\n",
    "#     for w in range(len(words)):\n",
    "#         for i in range(1, 6):\n",
    "#             file_mat=[]\n",
    "#             file_data=open(('feature/'+peeps[p]+'/'+peeps[p]+'_'+words[w]+str(i)+'.fea'), encoding = \"ISO-8859-1\").read().strip().split('\\n')\n",
    "#             file_mat = np.array([np.float_(line.split(',')) for line in file_data])\n",
    "#             features[peeps[p]][words[w]][i]=file_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['dg']['cnn'][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Testing-Training Data (Speaker Independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sp_independent=defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "train_sp_independent=defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "data_per_word_id=[]\n",
    "\n",
    "peeps_id=['dg', 'ls', 'yx']\n",
    "for x in range(len(words)):\n",
    "    w=words[x]\n",
    "    per_word=[]\n",
    "    for y in range(len(peeps_id)):\n",
    "        p=peeps_id[y]\n",
    "        for i in range(1, 6):\n",
    "            train_sp_independent[p][w][i]=features[p][w][i]\n",
    "            per_word.append(features[p][w][i])\n",
    "    data_per_word_id.append(per_word)\n",
    "    \n",
    "test_word_id=[]\n",
    "true_labels_id = []\n",
    "for x in range(len(words)):\n",
    "    w=words[x]\n",
    "    per_word=[]\n",
    "    for p in ['mh']:\n",
    "        for i in range(1, 6):\n",
    "            test_sp_independent[p][w][i]=features[p][w][i] \n",
    "            per_word.append(features[p][w][i])\n",
    "            true_labels_id.append(w)\n",
    "        test_word_id.append(per_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00040939 -0.00301437 -0.00775597 ... -0.00372121 -0.00549609\n",
      "  -0.00788802]\n",
      " [-0.0170444  -0.01566191 -0.01460274 ...  0.02594256  0.02252512\n",
      "   0.01776588]\n",
      " [ 0.01360221  0.00614428 -0.00132555 ...  0.00389542  0.00192771\n",
      "  -0.00021314]\n",
      " ...\n",
      " [ 0.03185443  0.02793878  0.02274978 ... -0.03198364 -0.0234382\n",
      "  -0.01449342]\n",
      " [-0.02182696 -0.02249822 -0.02328165 ... -0.00737874 -0.01098175\n",
      "  -0.01533739]\n",
      " [-0.01733734 -0.01173732 -0.00627662 ...  0.01767267  0.02134773\n",
      "   0.02001401]]\n"
     ]
    }
   ],
   "source": [
    "print(train_sp_independent['dg']['asr'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Testing-Training Data (Speaker Dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asr\n",
      "asr\n",
      "asr\n",
      "asr\n",
      "cnn\n",
      "cnn\n",
      "cnn\n",
      "cnn\n",
      "dnn\n",
      "dnn\n",
      "dnn\n",
      "dnn\n",
      "hmm\n",
      "hmm\n",
      "hmm\n",
      "hmm\n",
      "tts\n",
      "tts\n",
      "tts\n",
      "tts\n"
     ]
    }
   ],
   "source": [
    "train_sp_dependent=defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "test_sp_dependent=defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "data_per_word_d=[]\n",
    "\n",
    "for x in range(len(words)):\n",
    "    w=words[x]\n",
    "    per_word=[]\n",
    "    for y in range(len(peeps)):\n",
    "        p=peeps[y]\n",
    "        for i in range(1, 5):\n",
    "            train_sp_dependent[p][w][i]=features[p][w][i]\n",
    "            per_word.append(features[p][w][i])\n",
    "    data_per_word_d.append(per_word)\n",
    "        \n",
    "test_word_d=[]\n",
    "true_labels_d = []\n",
    "for x in range(len(words)):\n",
    "    w=words[x]\n",
    "    per_word=[]\n",
    "    for j in range(len(peeps)):\n",
    "        p=peeps[j]\n",
    "        for i in range(5, 6):\n",
    "            test_sp_dependent[p][w][i]=features[p][w][i]\n",
    "            per_word.append(np.array(features[p][w][i]))\n",
    "            print(w)\n",
    "            true_labels_d.append(w)\n",
    "        test_word_d.append(per_word)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asr',\n",
       " 'asr',\n",
       " 'asr',\n",
       " 'asr',\n",
       " 'cnn',\n",
       " 'cnn',\n",
       " 'cnn',\n",
       " 'cnn',\n",
       " 'dnn',\n",
       " 'dnn',\n",
       " 'dnn',\n",
       " 'dnn',\n",
       " 'hmm',\n",
       " 'hmm',\n",
       " 'hmm',\n",
       " 'hmm',\n",
       " 'tts',\n",
       " 'tts',\n",
       " 'tts',\n",
       " 'tts']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_labels_d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Recorded Samples: Test Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['asr', 'cnn', 'dnn', 'hmm', 'tts']\n",
    "self_features=defaultdict(lambda: defaultdict(partial(np.ndarray, 0)))\n",
    "test_self=[]=defaultdict(lambda: defaultdict(partial(np.ndarray, 0)))\n",
    "\n",
    "for w in words:\n",
    "    for i in range(1, 6):\n",
    "        self_features[w][i] = open(('Self-Recorded/Self Features/ak_'+w+str(i)+'.txt'), encoding = \"ISO-8859-1\").read().split('\\n')\n",
    "\n",
    "\n",
    "# for x, y in self_features.items():\n",
    "#     for z in y:\n",
    "#         l=z.replace('\\t', ',')[:-1].split()\n",
    "#         test_self.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Parameters: $\\pi$, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create B below:\n",
    "means_per_word_id=[]\n",
    "for x in range(len(words)):\n",
    "    w=words[x]\n",
    "    for y in range(len(peeps_id)):\n",
    "        p=peeps_id[y]\n",
    "        for i in range(1, 6):\n",
    "            if len(train_sp_independent[p][w][i]) != 0:\n",
    "                temp_mean=np.mean(train_sp_independent[p][w][i], axis=0)\n",
    "                #print(temp_mean)\n",
    "    means_per_word_id.append(temp_mean)\n",
    "    \n",
    "cov_per_word_id=[]\n",
    "for x in range(len(words)):\n",
    "    w=words[x]\n",
    "    for y in range(len(peeps_id)):\n",
    "        p=peeps_id[y]\n",
    "        for i in range(1, 6):\n",
    "            if len(train_sp_independent[p][w][i]) != 0:\n",
    "                temp_cov=np.cov(train_sp_independent[p][w][i].T)\n",
    "    cov_per_word_id.append(temp_cov)\n",
    "# # #Initializing b_j(x):\n",
    "# for i in range(0, 5):\n",
    "#     for j in range (0, 15):\n",
    "#         gauss[i][j] = stats.multivariate_normal(mean=means_per_word_id[i], cov=cov_per_word_id[i]).pdf(data_per_word_id[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_per_word_d[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create B below:\n",
    "means_per_word_d=[]\n",
    "for x in range(len(words)):\n",
    "    w=words[x]\n",
    "    for y in range(len(peeps)):\n",
    "        p=peeps[y]\n",
    "        for i in range(1, 5):\n",
    "            if len(train_sp_dependent[p][w][i]) != 0:\n",
    "                temp_mean=np.mean(train_sp_dependent[p][w][i], axis=0)\n",
    "                #print(temp_mean)\n",
    "    means_per_word_d.append(temp_mean)\n",
    "    \n",
    "cov_per_word_d=[]\n",
    "for x in range(len(words)):\n",
    "    w=words[x]\n",
    "    for y in range(len(peeps)):\n",
    "        p=peeps[y]\n",
    "        for i in range(1, 6):\n",
    "            if len(train_sp_dependent[p][w][i]) != 0:\n",
    "                temp_cov=np.cov(train_sp_dependent[p][w][i].T)\n",
    "    cov_per_word_d.append(temp_cov)\n",
    "# # #Initializing b_j(x):\n",
    "# for i in range(0, 5):\n",
    "#     for j in range (0, 15):\n",
    "#         gauss[i][j] = stats.multivariate_normal(mean=means_per_word_id[i], cov=cov_per_word_id[i]).pdf(data_per_word_id[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_per_word_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining GaussianHMM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GHMM:\n",
    "    def __init__(self, mean, cov):\n",
    "        states = [0,1,2,3,4]\n",
    "        self.N = len(states)-1\n",
    "        self.Sigma=[cov+0.5*np.identity(len(cov))]*self.N\n",
    "        self.mu=[mean]*self.N\n",
    "        \n",
    "        self.pi= [1/5, 1/5, 1/5, 1/5, 1/5] #Initial Probabilty Vector\n",
    "        self.A = [[0.8, 0.2, 0, 0, 0],\n",
    "             [0, 0.8, 0.2, 0, 0],\n",
    "             [0, 0, 0.8, 0.2, 0],   #Transition Probabilty Matrix\n",
    "             [0, 0, 0, 0.8, 0.2]]\n",
    "             #[0, 0, 0, 0, 1]]\n",
    "    \n",
    "    def train(self, X):\n",
    "        gamma, xi=self.E_step(X)\n",
    "        self.M_step(X, gamma, xi)\n",
    "    \n",
    "    def get_B(self, file): \n",
    "#         print(\"file: \\n\")\n",
    "#         print(file)\n",
    "#         print('----------')\n",
    "        T=len(file)\n",
    "        B=np.zeros((self.N,T))\n",
    "        for i in range(0,self.N):\n",
    "            for t in range(0,T):\n",
    "#                 print('m: \\n')\n",
    "#                 print(self.mu[i])\n",
    "#                 print(\"\\nS: \\n\")\n",
    "#                 print(self.Sigma[i])\n",
    "#                 print('\\n----------------\\n')\n",
    "#                 print('X: \\n')\n",
    "#                 print(file[t])\n",
    "#                 print('\\n-----------------------------------------\\n')\n",
    "                B[i,t]=stats.multivariate_normal(self.mu[i], self.Sigma[i]).pdf(file[t])\n",
    "#                 print(\"B: \\n\")\n",
    "#                 print(B[i][t])\n",
    "#                 print('----------')\n",
    "                #B[i,t]=stats.multivariate_normal(np.mean(file, axis=0)[i], np.cov(file.T)[i]).pdf(file[t])\n",
    "#         print(\"Bin: \\n\")\n",
    "#         print(B)\n",
    "#         print('----------')\n",
    "        return B\n",
    "    \n",
    "    def E_step(self, X):\n",
    "        N=self.N\n",
    "        L=len(X)\n",
    "        maxTsteps=max(len(xdata) for xdata in X)\n",
    "        Amat = np.array(self.A)\n",
    "        \n",
    "        gamma=np.zeros((N, maxTsteps, L))\n",
    "        xi=np.zeros((N, N, maxTsteps, L))\n",
    "        \n",
    "        for inum, f in enumerate(X):\n",
    "#             print('f: \\n')\n",
    "#             print(f)\n",
    "            B=self.get_B(f)\n",
    "#             print(\"B: \\n\")\n",
    "#             print(B)\n",
    "            T=len(f)\n",
    "            tildealpha=np.zeros((N,T))\n",
    "            tildebeta=np.zeros((N,T))\n",
    "            log_g = np.zeros((T))\n",
    "            baralpha = np.zeros((N,T))\n",
    "\n",
    "\n",
    "            for i in range(0,N):\n",
    "                baralpha[i,0]=self.pi[i]*B[i,0]\n",
    "            log_g[0] = np.log(np.sum(baralpha[:,0]))\n",
    "            tildealpha[:,0]=baralpha[:,0]/np.exp(log_g[0])\n",
    "\n",
    "            for t in range(1,T):\n",
    "                for i in range(0,N):\n",
    "#                     print('B:\\n')\n",
    "#                     print(B[i,t])\n",
    "#                     print('\\n tAlpha:\\n')\n",
    "#                     print(tildealpha[:,t-1])\n",
    "#                     print('\\n A:\\n')\n",
    "#                     print(Amat[:,i])\n",
    "#                     print('\\n t-B: \\n')\n",
    "#                     print(tildebeta[:,t+1]*B[:,t+1])\n",
    "#                     print('\\n inner: \\n')\n",
    "#                     print(np.inner(tildealpha[:,t-1],Amat[:,i]))\n",
    "#                     print('\\n balpha: \\n')\n",
    "#                     print(B[i,t]*np.inner(tildealpha[:,t-1],Amat[:,i]))\n",
    "#                     print('\\n----------------------------------------------\\n')\n",
    "                    baralpha[i,t]=B[i,t]*np.inner(tildealpha[:,t-1],Amat[:,i])\n",
    "                log_g[t] = np.log(np.sum(baralpha[:,t]))\n",
    "                tildealpha[:,t]=baralpha[:,t]/np.exp(log_g[t])\n",
    "\n",
    "            for i in range(0,N):\n",
    "                tildebeta[i,T-1] = 1/np.exp(log_g[T-1])\n",
    "\n",
    "            for t in range(T-2,-1,-1):\n",
    "                for i in range(0,N):\n",
    "                    tildebeta[i,t]=np.inner(Amat[i,0:N],tildebeta[:,t+1]*B[:,t+1])/np.exp(log_g[t+1])\n",
    "\n",
    "            for t in range(0,T):\n",
    "                gamma[:,t, inum] = tildealpha[:,t]*tildebeta[:,t]\n",
    "                gamma[:,t, inum] = gamma[:,t, inum]/np.sum(gamma[:,t, inum])\n",
    "    \n",
    "            for t in range(0,T):\n",
    "                for i in range(0,N):\n",
    "                    for j in range(0,N):\n",
    "                        xi[i,j,t,inum]=tildealpha[i,t]*Amat[i,j]\n",
    "                        if (t<T-1):\n",
    "                            if j==N:\n",
    "                                xi[i,j,t,inum]=0\n",
    "                            else:\n",
    "                                xi[i,j,t,inum] = xi[i,j,t,inum]*B[j,t+1]*tildebeta[j,t+1]\n",
    "                xi[:,:,t,inum]=xi[:,:,t,inum]/np.sum(xi[:,:,t,inum])\n",
    "                \n",
    "        return gamma, xi\n",
    "    \n",
    "    def M_step(self, X, gamma, xi):\n",
    "        N=self.N\n",
    "        L=len(X)\n",
    "        \n",
    "        for i in range(0,N):\n",
    "            for j in range(0,N):\n",
    "                self.A[i][j]=np.sum(xi[i,j,:])/np.sum(gamma[i,:])\n",
    "                \n",
    "        for i in range(0,N):\n",
    "            self.mu[i]=0\n",
    "            for l in range(L):\n",
    "                for t in range(len(X[l])):\n",
    "                    self.mu[i]+=X[l][t]*gamma[i, t, l]\n",
    "            self.mu[i]/=np.sum(gamma[i])\n",
    "            \n",
    "        for i in range(0,N):\n",
    "            self.Sigma[i]=0.5*np.identity(len(self.Sigma[i]))\n",
    "            for l in range(L):\n",
    "                for t in range(len(X[l])):\n",
    "                    self.Sigma[i] += gamma[i,t,l]*np.outer(X[l][t]-self.mu[i], X[l][t]-self.mu[i])\n",
    "            self.Sigma[i]/=np.sum(gamma[i])    \n",
    "                \n",
    "    def test(self, file):\n",
    "        B=self.get_B(file)\n",
    "#         print(\"B: \\n\")\n",
    "#         print(B)\n",
    "        T=len(file)\n",
    "        N=self.N\n",
    "        tildealpha=np.zeros((N,T))\n",
    "        log_g = np.zeros((T))\n",
    "        baralpha = np.zeros((N,T))\n",
    "        Amat = np.array(self.A)\n",
    "\n",
    "        for i in range(0,N):\n",
    "#             print(B[0,0])\n",
    "#             print('--------------------------')\n",
    "             baralpha[i,0]=self.pi[i]*B[i,0]\n",
    "        log_g[0] = np.log(np.sum(baralpha[:,0]))\n",
    "        tildealpha[:,0]=baralpha[:,0]/np.exp(log_g[0])\n",
    "\n",
    "        for t in range(1,T):\n",
    "            for i in range(0,N):\n",
    "                baralpha[i,t]=B[i,t]*np.inner(tildealpha[:,t-1],Amat[:,i])\n",
    "            log_g[t] = np.log(np.sum(baralpha[:,t]))\n",
    "            tildealpha[:,t]=baralpha[:,t]/np.exp(log_g[t])\n",
    "                \n",
    "        return sum(log_g)\n",
    "#         alpha = np.zeros((N,T))\n",
    "#         beta = np.zeros((N,T))\n",
    "#         gamma = np.zeros((N,T))\n",
    "#         xi = np.zeros((2*N,T))\n",
    "#         Amat = np.array(A)  # Convert to an np matrix so we can compute inner products\n",
    "#         for i in range(0,N):\n",
    "#             alpha[i,0]=pi[i]*B[i,0]\n",
    "#         for t in range(1,T):\n",
    "#             for i in range(0,N):\n",
    "#                 alpha[i,t]=B[i,t]*np.inner(alpha[:,t-1],Amat[:,i])\n",
    "#         for i in range(0,N):\n",
    "#             beta[i,T-1]=1\n",
    "#         for t in range(T-2,-1,-1):\n",
    "#             for i in range(0,N):\n",
    "#                 beta[i,t]=np.inner(Amat[i,0:N],beta[:,t+1]*B[:,t+1])\n",
    "#         for t in range(0,T):\n",
    "#             gamma[:,t]=alpha[:,t]*beta[:,t]\n",
    "#             gamma[:,t]=gamma[:,t]/np.sum(gamma[:,t])\n",
    "#         for t in range(0,T):\n",
    "#             for i in range(0,N):\n",
    "#                 for j in range(i,i+2):\n",
    "#                     xi[i+j,t]=alpha[i,t]*Amat[i,j]\n",
    "#                     if (t<T-1):\n",
    "#                         if j==N:\n",
    "#                             xi[i+j,t]=0\n",
    "#                         else:\n",
    "#                             xi[i+j,t] = xi[i+j,t]*B[j,t+1]*beta[j,t+1]\n",
    "#             xi[:,t]=xi[:,t]/np.sum(xi[:,t])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GaussianHMMs (Speaker Independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models_id=np.empty(len(words), dtype=np.object)\n",
    "# for i in range(len(words)):\n",
    "#     models_id[i]=GHMM(means_per_word_id[i], cov_per_word_id[i])\n",
    "    \n",
    "# for i in range(len(words)):\n",
    "#     models_id[i].train(data_per_word_id[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #predictions_id\n",
    "# predictions_id = [0] * len(words)*len(test_word_id[0])\n",
    "\n",
    "# for j in range(len(words)*len(test_word_id[0])):\n",
    "#     log_gs_id = [0] * len(words)\n",
    "#     for i in range(len(words)):\n",
    "# #         print('jnum: '+str(j))\n",
    "# #         print('file-in: \\n')\n",
    "# #         print('i: '+str(math.floor(j/len(test_word_id[0]))))\n",
    "# #         print('j: '+str(j%len(test_word_id[0])))\n",
    "# #         print(test_word_id[math.floor(j/len(test_word_id[0]))][j%len(test_word_id[0])])\n",
    "#         log_gs_id[i]=models_id[i].test(test_word_id[math.floor(j/len(test_word_id[0]))][j%len(test_word_id[0])])\n",
    "#     predictions_id[j]=words[np.argmax(log_gs_id)]\n",
    "    \n",
    "# same=0\n",
    "# for j in range(len(predictions_id)):\n",
    "#     print(predictions_id[j],', ',true_labels_id[j])\n",
    "#     if predictions_id[j]==true_labels_id[j]:\n",
    "#         same+=1\n",
    "# print(same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training GaussianHMMs (Speaker Dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-807cab27f1a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#     print('data: \\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#     print(len(data_per_word_d[i][16]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodels_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_per_word_d\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-82c95bd5bd4e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mM_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-82c95bd5bd4e>\u001b[0m in \u001b[0;36mE_step\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#             print('f: \\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m#             print(f)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mB\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;31m#             print(\"B: \\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#             print(B)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-82c95bd5bd4e>\u001b[0m in \u001b[0;36mget_B\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m#                 print(file[t])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;31m#                 print('\\n-----------------------------------------\\n')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0mB\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultivariate_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSigma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;31m#                 print(\"B: \\n\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m#                 print(B[i][t])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, mean, cov, allow_singular, seed)\u001b[0m\n\u001b[1;32m    355\u001b[0m         return multivariate_normal_frozen(mean, cov,\n\u001b[1;32m    356\u001b[0m                                           \u001b[0mallow_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m                                           seed=seed)\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_process_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mean, cov, allow_singular, seed, maxpts, abseps, releps)\u001b[0m\n\u001b[1;32m    725\u001b[0m         self.dim, self.mean, self.cov = self._dist._process_parameters(\n\u001b[1;32m    726\u001b[0m                                                             None, mean, cov)\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_PSD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmaxpts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0mmaxpts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/stats/_multivariate.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0;31m# Note that eigh takes care of array conversion, chkfinite,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# and assertion that the matrix is square.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_eigvalsh_to_eps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/scipy/linalg/decomp.py\u001b[0m in \u001b[0;36meigh\u001b[0;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0meigvals\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             w, v, info = evr(a1, uplo=uplo, jobz=_job, range=\"A\", il=1,\n\u001b[0;32m--> 387\u001b[0;31m                              iu=a1.shape[0], overwrite_a=overwrite_a)\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mlo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meigvals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models_d=np.empty(len(words), dtype=np.object)\n",
    "for i in range(len(words)):\n",
    "    models_d[i]=GHMM(means_per_word_d[i], cov_per_word_d[i])\n",
    "    \n",
    "for i in range(len(words)):\n",
    "#     print('data: \\n')\n",
    "#     print(len(data_per_word_d[i][16]))\n",
    "    models_d[i].train(data_per_word_d[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #predictions_d\n",
    "# predictions_d = [0] * len(words)*len(test_word_d[0])\n",
    "\n",
    "# for j in range(len(words)*len(test_word_d[0])):\n",
    "#     log_gs_d = [0] * len(words)\n",
    "#     for i in range(len(words)):\n",
    "# #         print('jnum: '+str(j))\n",
    "# #         print('file-in: \\n')\n",
    "# #         print('i: '+str(math.floor(j/len(test_word_d[0])))+'| j: '+str(j%len(test_word_d[0])))\n",
    "# #         print(test_word_d[math.floor(j/len(test_word_d[0]))][j%len(test_word_d[0])])\n",
    "#         log_gs_d[i]=models_d[i].test(test_word_d[math.floor(j/len(test_word_d[0]))][j%len(test_word_d[0])])\n",
    "#     predictions_d[j]=words[np.argmax(log_gs_d)]\n",
    "    \n",
    "# same=0\n",
    "# for j in range(len(predictions_d)):\n",
    "#     print(predictions_d[j],', ',true_labels_d[j])\n",
    "#     if predictions_d[j]==true_labels_d[j]:\n",
    "#         same+=1\n",
    "# print(same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.02682359, -0.02825837, -0.02909743, ...,  0.03330388,\n",
       "         0.03787388,  0.04118939],\n",
       "       [ 0.03095887,  0.03722286,  0.04382465, ..., -0.03011884,\n",
       "        -0.03203723, -0.03473858],\n",
       "       [ 0.02311902,  0.02759561,  0.02553632, ...,  0.00431988,\n",
       "         0.00119618, -0.00083926],\n",
       "       ...,\n",
       "       [ 0.01390739,  0.00145613,  0.01033041, ..., -0.02221057,\n",
       "        -0.01963782, -0.0224256 ],\n",
       "       [ 0.04677068,  0.04196013,  0.03599097, ...,  0.06350328,\n",
       "         0.05369636,  0.04061373],\n",
       "       [-0.01077211, -0.00550978, -0.01394606, ...,  0.0114735 ,\n",
       "         0.02031742,  0.0275134 ]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word_d[4][0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
