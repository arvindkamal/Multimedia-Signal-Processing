{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Problem #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports Here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from numpy import linalg\n",
    "import wave\n",
    "from scipy.io import wavfile\n",
    "from scipy import signal\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "data= {}\n",
    "fs={}\n",
    "peeps={'dg', 'ls', 'mh', 'yx'}\n",
    "words={'asr', 'cnn', 'dnn', 'hmm', 'tts'}\n",
    "\n",
    "for p in peeps:\n",
    "    for w in words:\n",
    "        for i in range(1, 6):\n",
    "            wav_r=wave.open('data/'+p+'/'+p+'_'+w+str(i)+'.wav', 'rb')\n",
    "            fs[p, w, i], x = wavfile.read('data/'+p+'/'+p+'_'+w+str(i)+'.wav')\n",
    "            if wav_r.getnchannels()==2:\n",
    "                data[p, w, i]=x[:, 0]\n",
    "            else:\n",
    "                data[p, w, i]=x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "peeps={'dg', 'ls', 'mh', 'yx'}\n",
    "words={'asr', 'cnn', 'dnn', 'hmm', 'tts'}\n",
    "\n",
    "for p in peeps:\n",
    "    for w in words:\n",
    "        for i in range(1, 6):\n",
    "            file_mat=[]\n",
    "            file_data=open(('feature/'+p+'/'+p+'_'+w+str(i)+'.fea'), encoding = \"ISO-8859-1\").read().strip().split('\\n')\n",
    "            file_mat = np.array([np.float_(line.split(',')) for line in file_data])\n",
    "            features[p][w][i]=file_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features['dg']['cnn'][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Testing-Training Data (Speaker Independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sp_independent=defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "train_sp_independent=defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "data_per_word_id=[]\n",
    "\n",
    "for w in words:\n",
    "    per_word=[]\n",
    "    c=0\n",
    "    for p in {'dg', 'ls', 'yx'}:\n",
    "        for i in range(1, 6):\n",
    "            train_sp_independent[p][w][i]=features[p][w][i]\n",
    "            per_word.append(features[p][w][i])\n",
    "    data_per_word_id.append(per_word)\n",
    "           \n",
    "for w in words:\n",
    "    per_word=[]\n",
    "    for p in {'mh'}:\n",
    "        for i in range(0, 5):\n",
    "            test_sp_independent[p][w][i]=features[p][w][i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_per_word_id[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Testing-Training Data (Speaker Dependent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sp_dependent=defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "test_sp_dependent=defaultdict(lambda: defaultdict(lambda: defaultdict(partial(np.ndarray, 0))))\n",
    "data_per_word_d=[]\n",
    "\n",
    "for w in words:\n",
    "    per_word=[]\n",
    "    c=0\n",
    "    for p in {'dg', 'ls', 'mh', 'yx'}:\n",
    "        for i in range(1, 5):\n",
    "            train_sp_dependent[p][w][i]=features[p][w][i]\n",
    "            per_word.append(np.array(features[p, w, i]))\n",
    "    data_per_word_d.append(np.array(per_word))\n",
    "            \n",
    "for w in words:\n",
    "    per_word=[]\n",
    "    for p in {'dg', 'ls', 'mh', 'yx'}:\n",
    "        for i in range(5, 6):\n",
    "            test_sp_dependent[p][w][i]=features[p][w][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self Recorded Samples: Test Feature Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words={'asr', 'cnn', 'dnn', 'hmm', 'tts'}\n",
    "self_features=defaultdict(lambda: defaultdict(partial(np.ndarray, 0)))\n",
    "test_self=[]=defaultdict(lambda: defaultdict(partial(np.ndarray, 0)))\n",
    "\n",
    "for w in words:\n",
    "    for i in range(1, 6):\n",
    "        self_features[w][i] = open(('Self-Recorded/Self Features/ak_'+w+str(i)+'.txt'), encoding = \"ISO-8859-1\").read().split('\\n')\n",
    "\n",
    "\n",
    "# for x, y in self_features.items():\n",
    "#     for z in y:\n",
    "#         l=z.replace('\\t', ',')[:-1].split()\n",
    "#         test_self.append(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HMM Parameters: $\\pi$, A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [1,2,3,4,5]\n",
    "N = len(states)-1\n",
    "pi_true= [1/5, 1/5, 1/5, 1/5, 1/5] #Initial Probabilty Vector\n",
    "\n",
    "A = [[0.8, 0.2, 0, 0, 0],\n",
    "     [0, 0.8, 0.2, 0, 0],\n",
    "     [0, 0, 0.8, 0.2, 0],   #Transition Probabilty Matrix\n",
    "     [0, 0, 0, 0.8, 0.2],\n",
    "     [0, 0, 0, 0, 1]]\n",
    "\n",
    "#Create B below:\n",
    "means_per_word_id=[]\n",
    "for w in words:\n",
    "    for p in {'dg', 'ls', 'yx'}:\n",
    "        for i in range(1, 6):\n",
    "            if len(train_sp_independent[p][w][i]) != 0:\n",
    "                temp_mean=np.mean(train_sp_independent[p][w][i], axis=0)\n",
    "    means_per_word_id.append(temp_mean)\n",
    "    \n",
    "cov_per_word_id=[]\n",
    "for w in words:\n",
    "    for p in {'dg', 'ls', 'yx'}:\n",
    "        for i in range(1, 6):\n",
    "            if len(train_sp_independent[p][w][i]) != 0:\n",
    "                temp_cov=np.cov(train_sp_independent[p][w][i].T)\n",
    "    cov_per_word_id.append(temp_cov)\n",
    "    \n",
    "\n",
    "# # #Initializing b_j(x):\n",
    "# for i in range(0, 5):\n",
    "#     for j in range (0, 15):\n",
    "#         gauss[i][j] = stats.multivariate_normal(mean=means_per_word_id[i], cov=cov_per_word_id[i]).pdf(data_per_word_id[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_Step_1(self, N, T, mu, Sigma, X): \n",
    "    T=len(X)\n",
    "    B=np.zeros((N,T))\n",
    "    for t in range(0,T):\n",
    "        for i in range(0,N):\n",
    "            B[i,t]=stats.multivariate_normal(mu[i],Sigma[i]).pdf(X[t])\n",
    "# states=[0, 1, 2, 3, 4]\n",
    "# Q = list(np.random.choice(states,1,p=pi_true))\n",
    "# q = Q[-1]\n",
    "# X = []\n",
    "# while q < 5:\n",
    "#     print(q)\n",
    "#     xt = stats.multivariate_normal.rvs(mean=mu_true[q],cov=np.diag(sigsq_true[q]))\n",
    "#     X.append(xt)\n",
    "#     Q.extend(np.random.choice(states,1,p=A_true[q]))\n",
    "#     q = Q[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_step_2(self, N, T, B):\n",
    "    alpha = np.zeros((N,T))\n",
    "    beta = np.zeros((N,T))\n",
    "    gamma = np.zeros((N,T))\n",
    "    xi = np.zeros((2*N,T))\n",
    "    Amat = np.array(A)  # Convert to an np matrix so we can compute inner products\n",
    "    for i in range(0,N):\n",
    "        alpha[i,0]=pi[i]*B[i,0]\n",
    "    for t in range(1,T):\n",
    "        for i in range(0,N):\n",
    "            alpha[i,t]=B[i,t]*np.inner(alpha[:,t-1],Amat[:,i])\n",
    "    for i in range(0,N):\n",
    "        beta[i,T-1]=1\n",
    "    for t in range(T-2,-1,-1):\n",
    "        for i in range(0,N):\n",
    "            beta[i,t]=np.inner(Amat[i,0:N],beta[:,t+1]*B[:,t+1])\n",
    "    for t in range(0,T):\n",
    "        gamma[:,t]=alpha[:,t]*beta[:,t]\n",
    "        gamma[:,t]=gamma[:,t]/np.sum(gamma[:,t])\n",
    "    for t in range(0,T):\n",
    "        for i in range(0,N):\n",
    "            for j in range(i,i+2):\n",
    "                xi[i+j,t]=alpha[i,t]*Amat[i,j]\n",
    "                if (t<T-1):\n",
    "                    if j==N:\n",
    "                        xi[i+j,t]=0\n",
    "                    else:\n",
    "                        xi[i+j,t] = xi[i+j,t]*B[j,t+1]*beta[j,t+1]\n",
    "        xi[:,t]=xi[:,t]/np.sum(xi[:,t])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
